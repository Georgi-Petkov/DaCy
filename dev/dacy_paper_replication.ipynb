{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('dacy': virtualenvwrapper)"
  },
  "interpreter": {
   "hash": "3306719ab0e316992d1a1835936bef6bb94c1674b49ab3d645e902d91649e7a1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Augmentation\n",
    "This notebook recreates Table X from the paper XX and illustrates how to use the augmenters and scoring functions included in DaCy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^should be true if not: \n",
    "#!pip install spacy[cuda102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os # assuming we are located in dacy github repo\n",
    "#os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dacy if not already installed\n",
    "#!pip install -r requirements.txt # assumed version 1.0.0 of dacy\n",
    "# or using\n",
    "#!pip install dacy\n",
    "\n",
    "# download relevant spacy models\n",
    "#!python -m spacy download da_core_news_sm\n",
    "#!python -m spacy download da_core_news_md\n",
    "#!python -m spacy download da_core_news_lg\n",
    "\n",
    "# download danlp dependencies\n",
    "#!pip install danlp==0.0.11 \n",
    "#!pip install transformers==3.5.1 --no-deps # for DaNLP\n",
    "#!pip install NERDA"
   ]
  },
  {
   "source": [
    "# The dataset: DaNE\n",
    "Start off by loading the test set of the DaNE dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dacy.datasets import dane\n",
    "test = dane(splits=[\"test\"])"
   ]
  },
  {
   "source": [
    "# Augmenters\n",
    "\n",
    "Create a list of augmenters we wish to apply to our model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training.augment import create_lower_casing_augmenter, dont_augment\n",
    "from dacy.augmenters import create_pers_augmenter, create_keyboard_augmenter, create_æøå_augmenter, create_remove_spacing_augmenter\n",
    "from dacy.datasets import danish_names, muslim_names, female_names, male_names\n",
    "\n",
    "# randomly augment names\n",
    "dk_name_dict = danish_names()\n",
    "muslim_name_dict = muslim_names()\n",
    "f_name_dict = female_names()\n",
    "m_name_dict = male_names()\n",
    "\n",
    "dk_aug = create_pers_augmenter(dk_name_dict, force_size=True, keep_name=False, patterns = [\"fn\", \"fn,ln\", \"fn,ln,ln\"])\n",
    "muslim_aug = create_pers_augmenter(muslim_name_dict, force_size=True, keep_name=False,  patterns = [\"fn\", \"fn,ln\", \"fn,ln,ln\"])\n",
    "f_aug = create_pers_augmenter(dk_name_dict, force_size=True, keep_name=False,  patterns = [\"fn\", \"fn,ln\", \"fn,ln,ln\"])\n",
    "m_aug = create_pers_augmenter(muslim_name_dict, force_size=True, keep_name=False,  patterns = [\"fn\", \"fn,ln\", \"fn,ln,ln\"])\n",
    "punct_aug = create_pers_augmenter(muslim_name_dict, force_size=False, keep_name=True, patterns = [\"abbpunct\"])\n",
    "\n",
    "\n",
    "# randomly change 5%/15% of characters to a neighbouring key\n",
    "keyboard_aug_02 = create_keyboard_augmenter(doc_level=1, char_level=0.02, keyboard=\"QWERTY_DA\")\n",
    "keyboard_aug_05 = create_keyboard_augmenter(doc_level=1, char_level=0.05, keyboard=\"QWERTY_DA\")\n",
    "keyboard_aug_15 = create_keyboard_augmenter(doc_level=1, char_level=0.15, keyboard=\"QWERTY_DA\")\n",
    "\n",
    "# Change æ=ae, ø=oe, å=aa\n",
    "æøå_aug = create_æøå_augmenter(doc_level=1, char_level=1)\n",
    "\n",
    "# lower case text\n",
    "lower_case_aug = create_lower_casing_augmenter(level=1)\n",
    "\n",
    "# spacing\n",
    "spacing_aug_05 = create_remove_spacing_augmenter(doc_level=1, spacing_level=0.05)\n",
    "spacing_aug = create_remove_spacing_augmenter(doc_level=1, spacing_level=1)\n",
    "\n",
    "n = 20\n",
    "               # augmenter   name               n rep\n",
    "augmenters = [(dont_augment, \"No augmentation\", 1),\n",
    "              (keyboard_aug_02, \"Keystroke errors 2%\", n),\n",
    "              (keyboard_aug_05, \"Keystroke errors 5%\", n), \n",
    "              (keyboard_aug_15, \"Keystroke errors 15%\", n), \n",
    "              (æøå_aug, \"Æøå Augmentation\",  1), \n",
    "              (lower_case_aug, \"Lowercase\" ,1), \n",
    "              (dk_aug, \"Danish names\", n), \n",
    "              (muslim_aug, \"Muslim names\", n),\n",
    "              (f_aug, \"Female names\", n),\n",
    "              (m_aug, \"Male names\", n),\n",
    "              (punct_aug, \"Abbreviated first names\", 1),\n",
    "              (spacing_aug_05, \"Spacing Augmention 5%\", n),\n",
    "              (spacing_aug, \"No Spacing\", 1)\n",
    "              ]\n"
   ]
  },
  {
   "source": [
    "# Apply functions\n",
    "Defining application functions for necessary models. No need to create one for SpaCy pipelines."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "def apply_bert_model(example, bert_model):\n",
    "    doc = example.predicted\n",
    "    # uses spacy tokenization\n",
    "    tokens, labels = bert_model.predict([t.text for t in example.reference])\n",
    "    ent = []\n",
    "    for i, t in enumerate(zip(doc, labels)):\n",
    "        token, label = t\n",
    "\n",
    "        # turn OOB labels into spans\n",
    "        if label == \"O\":\n",
    "            continue\n",
    "        iob, ent_type = label.split(\"-\")\n",
    "        if (i - 1 >= 0 and iob == \"I\" and labels[i - 1] == \"O\") or (\n",
    "            i == 0 and iob == \"I\"\n",
    "        ):\n",
    "            iob = \"B\"\n",
    "        if iob == \"B\":\n",
    "            start = i\n",
    "        if i + 1 >= len(labels) or labels[i + 1].split(\"-\")[0] != \"I\":\n",
    "            ent.append(Span(doc, start, i + 1, label=ent_type))\n",
    "    doc.set_ents(ent)\n",
    "    example.predicted = doc\n",
    "    return example\n",
    "\n",
    "def apply_nerda_model(example, bert_model):\n",
    "    doc = example.predicted\n",
    "    # uses spacy tokenization\n",
    "    labels = bert_model.predict([[t.text for t in example.predicted]]) # nerda requires it to be list of list of tokens\n",
    "    labels = labels[0]\n",
    "    ent = []\n",
    "    for i, t in enumerate(zip(doc, labels)):\n",
    "        token, label = t\n",
    "        # turn OOB labels into spans\n",
    "        if label == \"O\":\n",
    "            continue\n",
    "        iob, ent_type = label.split(\"-\")\n",
    "        if (i - 1 >= 0 and iob == \"I\" and labels[i - 1] == \"O\") or (\n",
    "            i == 0 and iob == \"I\"\n",
    "        ):\n",
    "            iob = \"B\"\n",
    "        if iob == \"B\":\n",
    "            start = i\n",
    "        if i + 1 >= len(labels) or labels[i + 1].split(\"-\")[0] != \"I\":\n",
    "            ent.append(Span(doc, start, i + 1, label=ent_type))\n",
    "    doc.set_ents(ent)\n",
    "    example.predicted = doc\n",
    "    return example\n",
    "    ### DaNLP's BERT model requires transformers==3.5.1 (install with pip install transformers==3.5.1 --no-deps)"
   ]
  },
  {
   "source": [
    "# Models\n",
    "A list of models to apply. To save memory the models are only loaded in one at a time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/kenneth/.Envs/dacy/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from danlp.models import load_bert_ner_model\n",
    "from NERDA.precooked import DA_BERT_ML\n",
    "\n",
    "from NERDA.precooked import DA_BERT_ML\n",
    "model = DA_BERT_ML()\n",
    "model.download_network()\n",
    "nerda.load_network()\n",
    "\n",
    "model_dict = {\n",
    "    \"spacy_small\" : \"da_core_news_sm\",\n",
    "    \"spacy_medium\": \"da_core_news_md\",\n",
    "    \"spacy_large\" : \"da_core_news_lg\",\n",
    "    \"dacy_small\" : \"da_dacy_small_tft-0.0.0\",\n",
    "    \"dacy_medium\" : \"da_dacy_medium_tft-0.0.0\",\n",
    "    \"dacy_large\" : \"da_dacy_large_tft-0.0.0\",\n",
    "    \"danlp_bert\" : load_bert_ner_model,\n",
    "    \"nerda_bert\" : model,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download the danlp you will have to set up a certificate:\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "source": [
    "# Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"robustness\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO]: Scoring model 'danlp_bert' using DaCy\n",
      "\t Running augmenter: No augmentation\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import pandas as pd\n",
    "import spacy    \n",
    "\n",
    "import dacy\n",
    "from dacy.score import score, n_sents_score\n",
    "\n",
    "\n",
    "for mdl in model_dict:\n",
    "    print(f\"[INFO]: Scoring model '{mdl}' using DaCy\")\n",
    "\n",
    "    # load model\n",
    "    if \"dacy\" in mdl:\n",
    "        apply_fn = dacy.load(model_dict[mdl])\n",
    "    elif \"spacy\" in mdl:\n",
    "        apply_fn = spacy.load(model_dict[mdl])\n",
    "    elif mdl == \"danlp_bert\":\n",
    "        bert = model_dict[mdl]()\n",
    "        apply_fn = partial(apply_bert_model, bert_model=bert)\n",
    "    else:\n",
    "        apply_fn = partial(apply_nerda_model, bert_model=nerda)\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    scores = []\n",
    "    for aug, nam, k in augmenters:\n",
    "        print(f\"\\t Running augmenter: {nam}\")\n",
    "\n",
    "        scores_ = score(corpus=test, apply_fn=apply_fn, augmenters=aug, k=k)\n",
    "        scores_[\"model\"] = mdl\n",
    "        scores_[\"augmenter\"] = nam\n",
    "        scores_[\"i\"] = i\n",
    "        scores.append(scores_)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    for n in [5, 10]:\n",
    "        scores_ = n_sents_score(n_sents = n, apply_fn=apply_fn)\n",
    "        scores_[\"model\"] = mdl\n",
    "        scores_[\"augmenter\"] = f\"Input size augmentation {n} sentences\"\n",
    "        scores_[\"i\"] = i + 1\n",
    "        scores.append(scores_)\n",
    "    scores = pd.concat(scores)\n",
    "\n",
    "    scores.to_csv(f\"robustness/{mdl}_augmentation_performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     ents_p    ents_r    ents_f  ents_per_type_PER_p  ents_per_type_PER_r  \\\n",
       "0  0.855072  0.634409  0.728395             0.917582             0.927778   \n",
       "\n",
       "   ents_per_type_PER_f  ents_per_type_MISC_p  ents_per_type_MISC_r  \\\n",
       "0             0.922652                   0.0                   0.0   \n",
       "\n",
       "   ents_per_type_MISC_f  ents_per_type_LOC_p  ents_per_type_LOC_r  \\\n",
       "0                   0.0             0.788991             0.895833   \n",
       "\n",
       "   ents_per_type_LOC_f  ents_per_type_ORG_p  ents_per_type_ORG_r  \\\n",
       "0             0.839024             0.821138             0.627329   \n",
       "\n",
       "   ents_per_type_ORG_f  k       model        augmenter  i  \n",
       "0             0.711268  0  danlp_bert  No augmentation  0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ents_p</th>\n      <th>ents_r</th>\n      <th>ents_f</th>\n      <th>ents_per_type_PER_p</th>\n      <th>ents_per_type_PER_r</th>\n      <th>ents_per_type_PER_f</th>\n      <th>ents_per_type_MISC_p</th>\n      <th>ents_per_type_MISC_r</th>\n      <th>ents_per_type_MISC_f</th>\n      <th>ents_per_type_LOC_p</th>\n      <th>ents_per_type_LOC_r</th>\n      <th>ents_per_type_LOC_f</th>\n      <th>ents_per_type_ORG_p</th>\n      <th>ents_per_type_ORG_r</th>\n      <th>ents_per_type_ORG_f</th>\n      <th>k</th>\n      <th>model</th>\n      <th>augmenter</th>\n      <th>i</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.855072</td>\n      <td>0.634409</td>\n      <td>0.728395</td>\n      <td>0.917582</td>\n      <td>0.927778</td>\n      <td>0.922652</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.788991</td>\n      <td>0.895833</td>\n      <td>0.839024</td>\n      <td>0.821138</td>\n      <td>0.627329</td>\n      <td>0.711268</td>\n      <td>0</td>\n      <td>danlp_bert</td>\n      <td>No augmentation</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "\n",
    "scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}