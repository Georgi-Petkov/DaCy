{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('NLP': virtualenvwrapper)"
  },
  "interpreter": {
   "hash": "2136a9c3637fd160483224d7922e48bf03b650be5dff26724a0c1f8d1279953b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Augmentation\n",
    "This notebook recreates Table X from the paper XX and illustrates how to use the augmenters and scoring functions included in DaCy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # assuming we are located in dacy\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt # assumed version 1.0.0 of dac\n",
    "#!pip install transformers==3.5.1 --no-deps # for DaNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import dacy\n",
    "from dacy.augmenters import create_pers_augmenter, create_keyboard_augmenter, create_æøå_augmenter\n",
    "from dacy.datasets import danish_names, muslim_names\n",
    "from dacy.score import score, n_sents_score\n",
    "\n",
    "import spacy\n",
    "from spacy.training.augment import create_lower_casing_augmenter, dont_augment\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "source": [
    "# The dataset: DaNE\n",
    "Start off by loading the test set of the DaNE dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dacy.datasets.dane(splits=[\"test\"])"
   ]
  },
  {
   "source": [
    "# Augmenters\n",
    "\n",
    "Create a list of augmenters we wish to apply to our model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly augment names\n",
    "dk_name_dict = danish_names()\n",
    "muslim_name_dict = muslim_names()\n",
    "f_name_dict = female_names()\n",
    "m_name_dict = male_names()\n",
    "\n",
    "dk_aug = create_pers_augmenter(dk_name_dict, force_size=True, keep_name=False)\n",
    "muslim_aug = create_pers_augmenter(muslim_name_dict, force_size=True, keep_name=False)\n",
    "f_aug = create_pers_augmenter(dk_name_dict, force_size=True, keep_name=False)\n",
    "m_aug = create_pers_augmenter(muslim_name_dict, force_size=True, keep_name=False)\n",
    "punct_aug = create_pers_augmenter(muslim_name_dict, force_size=False, keep_name=True, pattern = [\"abbpunct\"])\n",
    "\n",
    "\n",
    "# randomly change 5%/15% of characters to a neighbouring key\n",
    "keyboard_aug_05 = create_keyboard_augmenter(doc_level=1, char_level=0.05, keyboard=\"QWERTY_DA\")\n",
    "keyboard_aug_15 = create_keyboard_augmenter(doc_level=1, char_level=0.15, keyboard=\"QWERTY_DA\")\n",
    "\n",
    "# Change æ=ae, ø=oe, å=aa\n",
    "æøå_aug = create_æøå_augmenter(doc_level=1, char_level=1)\n",
    "\n",
    "# lower case text\n",
    "lower_case_aug = create_lower_casing_augmenter(level=1)\n",
    "\n",
    "n = 20\n",
    "               # augmenter   name               n rep\n",
    "augmenters = [(dont_augment, \"No augmentation\", 1),\n",
    "              (keyboard_aug_05, \"Keyboard augmentation 0.05%\", n), \n",
    "              (keyboard_aug_15, \"Keyboard augmentation 0.15%\", n), \n",
    "              (æøå_aug, \"Æøå augmentation\",  1), \n",
    "              (lower_case_aug, \"Lowercase augmentation\" ,1), \n",
    "              (dk_aug, \"Danish names augmentation\", n), \n",
    "              (muslim_aug, \"Muslim names augmentation\", n),\n",
    "              (f_aug, \"Female names augmentation\", n)\n",
    "              (m_aug, \"Male names augmentation\", n)\n",
    "              (punct_aug, \"Abbreviated names augmentation\", 1)]\n"
   ]
  },
  {
   "source": [
    "# Apply functions\n",
    "Defining application functions for necessary models. No need to create one for SpaCy pipelines."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "def apply_bert_model(example, bert_model):\n",
    "    doc = example.predicted\n",
    "    # uses spacy tokenization\n",
    "    tokens, labels = bert_model.predict([t.text for t in example.predicted])\n",
    "    ent = []\n",
    "    for i, t in enumerate(zip(doc, labels)):\n",
    "        token, label = t\n",
    "\n",
    "        # turn OOB labels into spans\n",
    "        if label == \"O\":\n",
    "            continue\n",
    "        iob, ent_type = label.split(\"-\")\n",
    "        if (i - 1 >= 0 and iob == \"I\" and labels[i - 1] == \"O\") or (\n",
    "            i == 0 and iob == \"I\"\n",
    "        ):\n",
    "            iob = \"B\"\n",
    "        if iob == \"B\":\n",
    "            start = i\n",
    "        if i + 1 >= len(labels) or labels[i + 1].split(\"-\")[0] != \"I\":\n",
    "            ent.append(Span(doc, start, i + 1, label=ent_type))\n",
    "    doc.set_ents(ent)\n",
    "    example.predicted = doc\n",
    "    return example\n",
    "\n",
    "def apply_nerda_model(example, bert_model):\n",
    "    doc = example.predicted\n",
    "    # uses spacy tokenization\n",
    "    labels = bert_model.predict([[t.text for t in example.predicted]]) # nerda requires it to be list of list of tokens\n",
    "    labels = labels[0]\n",
    "    ent = []\n",
    "    for i, t in enumerate(zip(doc, labels)):\n",
    "        token, label = t\n",
    "        # turn OOB labels into spans\n",
    "        if label == \"O\":\n",
    "            continue\n",
    "        iob, ent_type = label.split(\"-\")\n",
    "        if (i - 1 >= 0 and iob == \"I\" and labels[i - 1] == \"O\") or (\n",
    "            i == 0 and iob == \"I\"\n",
    "        ):\n",
    "            iob = \"B\"\n",
    "        if iob == \"B\":\n",
    "            start = i\n",
    "        if i + 1 >= len(labels) or labels[i + 1].split(\"-\")[0] != \"I\":\n",
    "            ent.append(Span(doc, start, i + 1, label=ent_type))\n",
    "    doc.set_ents(ent)\n",
    "    example.predicted = doc\n",
    "    return example\n",
    "    ### DaNLP's BERT model requires transformers==3.5.1 (install with pip install transformers==3.5.1 --no-deps)"
   ]
  },
  {
   "source": [
    "# Models\n",
    "A list of models to apply. To save memory the models are only loaded in one at a time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.models import load_bert_ner_model\n",
    "from NERDA.precooked import DA_BERT_ML\n",
    "\n",
    "model_dict = {\n",
    "    #\"spacy_small\" : \"da_core_news_sm\",\n",
    "    #\"spacy_medium\": \"da_core_news_md\",\n",
    "    #\"spacy_large\" : \"da_core_news_lg\",\n",
    "    #\"dacy_small\" : \"da_dacy_small_tft-0.0.0\",\n",
    "    #\"dacy_medium\" : \"da_dacy_medium_tft-0.0.0\",\n",
    "    #\"dacy_large\" : \"da_dacy_large_tft-0.0.0\",\n",
    "    #\"danlp_bert\" : load_bert_ner_model,\n",
    "    \"nerda_bert\" : DA_BERT_ML,\n",
    "}\n"
   ]
  },
  {
   "source": [
    "# Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"robustness\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO]: Scoring model 'nerda_bert' using DaCy\n",
      "Device automatically set to: cpu\n",
      "\t Running augmenter: No augmentation\n",
      "\t Running augmenter: Keyboard augmentation 0.05%\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 73, in default_collate\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"/Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 73, in <dictcomp>\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"/Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 53, in default_collate\n    storage = elem.storage()._new_shared(numel)\n  File \"/Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages/torch/storage.py\", line 135, in _new_shared\n    return cls._new_using_filename(size)\nRuntimeError: std::exception at ../torch/lib/libshm/core.cpp:99\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-558aaf5ee725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maugmenters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\t Running augmenter: {nam}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mscores_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapply_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmenters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mscores_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Github/DaCy/dacy/score/score.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(corpus, apply_fn, score_fn, augmenters, k, nlp, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mscores_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mscores_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Github/DaCy/dacy/score/score.py\u001b[0m in \u001b[0;36m__score\u001b[0;34m(augmenter)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mscores_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mapply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscore_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Github/DaCy/dacy/score/score.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mscores_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mapply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscore_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-67ca1fc06eeb>\u001b[0m in \u001b[0;36mapply_nerda_model\u001b[0;34m(example, bert_model)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# uses spacy tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# nerda requires it to be list of list of tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0ment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/NERDA/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentences, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mpredicted\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mentity\u001b[0m \u001b[0mper\u001b[0m \u001b[0mword\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \"\"\"\n\u001b[0;32m--> 282\u001b[0;31m         return predict(network = self.network, \n\u001b[0m\u001b[1;32m    283\u001b[0m                        \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                        \u001b[0mtransformer_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/NERDA/predictions.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(network, sentences, transformer_tokenizer, transformer_config, max_len, device, tag_encoder, tag_outside, batch_size, num_workers, return_tensors, pad_sequences)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 73, in default_collate\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"/Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 73, in <dictcomp>\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"/Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 53, in default_collate\n    storage = elem.storage()._new_shared(numel)\n  File \"/Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages/torch/storage.py\", line 135, in _new_shared\n    return cls._new_using_filename(size)\nRuntimeError: std::exception at ../torch/lib/libshm/core.cpp:99\n"
     ]
    }
   ],
   "source": [
    "for mdl in model_dict:\n",
    "    print(f\"[INFO]: Scoring model '{mdl}' using DaCy\")\n",
    "\n",
    "    # load model\n",
    "    if \"dacy\" in mdl:\n",
    "        apply_fn = dacy.load(model_dict[mdl])\n",
    "    elif \"spacy\" in mdl:\n",
    "        apply_fn = spacy.load(model_dict[mdl])\n",
    "    elif mdl == \"danlp_bert\":\n",
    "        bert = model_dict[mdl]()\n",
    "        apply_fn = partial(apply_bert_model, bert_model=bert)\n",
    "    else:\n",
    "        bert = model_dict[mdl]()\n",
    "        apply_fn = partial(apply_nerda_model, bert_model=bert)\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    scores = []\n",
    "    for aug, nam, k in augmenters:\n",
    "        print(f\"\\t Running augmenter: {nam}\")\n",
    "\n",
    "        scores_ = score(corpus=test, apply_fn=apply_fn, augmenters=aug, k=k)\n",
    "        scores_[\"model\"] = mdl\n",
    "        scores_[\"augmenter\"] = nam\n",
    "        scores_[\"i\"] = i\n",
    "        scores.append(scores_)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    scores_ = n_sents_scorer(n_sents = [5, 10], apply_fn=apply_fn)\n",
    "    scores_[\"model\"] = mdl\n",
    "    scores_[\"augmenter\"] = \"Input size augmentation\"\n",
    "    scores_[\"i\"] = i + 1\n",
    "    scores.append(scores_)\n",
    "    scores = pd.concat(scores)\n",
    "\n",
    "    scores.to_csv(f\"robustness/{mdl}_augmentation_performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}