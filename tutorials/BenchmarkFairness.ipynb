{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BenchmarkFairness.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yVblU-A-ik5T",
        "rqLCJBocjSAi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jjrwfowJ-SR"
      },
      "source": [
        "# Benchmarking fairness on test DaNE dataset\n",
        "Working progress\n",
        "\n",
        "**Disclaimer:** The name dataset with ethnicities is from 2010 and only has female names with other ehtnicities than Danish.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjR89kX3aSAo",
        "outputId": "886a16ba-7868-407b-b526-ea8c562a047b"
      },
      "source": [
        "#!pip install git+https://github.com/KennethEnevoldsen/DaCy\n",
        "!pip install conllu\n",
        "!pip install tabulate\n",
        "!pip install seqeval\n",
        "!pip install xlrd\n",
        "!pip install openpyxl\n",
        "!pip install transformers==3.6.0"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.\u001b[0m\n",
            "Requirement already satisfied: conllu in /Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages (4.4)\n",
            "\u001b[33mWARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.\u001b[0m\n",
            "Requirement already satisfied: tabulate in /Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages (0.8.7)\n",
            "\u001b[33mWARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.\u001b[0m\n",
            "Requirement already satisfied: seqeval in /Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages (from seqeval) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (0.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (2.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
            "\u001b[33mWARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.\u001b[0m\n",
            "Requirement already satisfied: xlrd in /Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages (2.0.1)\n",
            "\u001b[33mWARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.\u001b[0m\n",
            "Requirement already satisfied: openpyxl in /Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages (3.0.7)\n",
            "Requirement already satisfied: et-xmlfile in /Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages (from openpyxl) (1.0.1)\n",
            "\u001b[33mWARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.\u001b[0m\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/transformers/\u001b[0m\n",
            "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/transformers/\u001b[0m\n",
            "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/transformers/\u001b[0m\n",
            "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/transformers/\u001b[0m\n",
            "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/transformers/\u001b[0m\n",
            "Could not fetch URL https://pypi.org/simple/transformers/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/transformers/ (Caused by SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")) - skipping\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement transformers==3.5.1\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for transformers==3.5.1\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc4Zcn4Tj6f5"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-teNWhCakGj2"
      },
      "source": [
        "from tabulate import tabulate\n",
        "import numpy as np\n",
        "from seqeval.metrics import classification_report, f1_score\n",
        "\n",
        "\n",
        "def f1_class(k, true, pred):\n",
        "    tp = np.sum(np.logical_and(pred == k, true == k))\n",
        "\n",
        "    fp = np.sum(np.logical_and(pred == k, true != k))\n",
        "    fn = np.sum(np.logical_and(pred != k, true == k))\n",
        "    if tp == 0:\n",
        "        return 0\n",
        "    recall = tp / (tp + fp)\n",
        "    precision = tp / (tp + fn)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return precision, recall, f1\n",
        "\n",
        "\n",
        "def f1_report(true, pred, modelname=\"\", dataname=\"\", word_level=False, bio=False):\n",
        "\n",
        "    if bio:\n",
        "        return classification_report(true, pred, digits=4)\n",
        "\n",
        "    if word_level:\n",
        "        true = [tag for sent in true for tag in sent]\n",
        "        pred = [tag for sent in pred for tag in sent]\n",
        "\n",
        "    data_b = []\n",
        "    data_a = []\n",
        "    headers_b = [\"{} // {} \".format(modelname, dataname), 'Class', 'Precision', 'Recall', 'F1', 'support']\n",
        "    headers_a = ['Accuracy', 'Avg-f1', 'Weighted-f1', '', '']\n",
        "    aligns_b = ['left', 'left', 'center', 'center', 'center']\n",
        "\n",
        "    true = np.array(true)\n",
        "    pred = np.array(pred)\n",
        "    acc = np.sum(true == pred) / len(true)\n",
        "\n",
        "    n = len(np.unique(true))\n",
        "    avg = 0\n",
        "    wei = 0\n",
        "    for c in np.unique(true):\n",
        "        precision, recall, f1 = f1_class(c, pred, true)\n",
        "        avg += f1 / n\n",
        "        wei += f1 * (np.sum(true == c) / len(true))\n",
        "\n",
        "        data_b.append(['', c, round(precision, 4), round(recall, 4), round(f1, 4)])\n",
        "    data_b.append(['', '', '', '', ''])\n",
        "    data_b.append(headers_a)\n",
        "    data_b.append([round(acc, 4), round(avg, 4), round(wei, 4), '', ''])\n",
        "    print()\n",
        "    print(tabulate(data_b, headers=headers_b, colalign=aligns_b), '\\n')\n",
        "    return data_b"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA_9_Y_ykUwg"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zk6B5CHaWyg"
      },
      "source": [
        "from danlp.datasets import DDT\n",
        "import pandas as pd\n",
        "import copy"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e8GWRRKtxps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee16605-b60d-4d7f-943c-92da346a9d46"
      },
      "source": [
        "def is_misc(ent: str):\n",
        "    if len(ent) < 4:\n",
        "        return False\n",
        "    return ent[-4:] == 'MISC'\n",
        "\n",
        "def isolate_per(se: list):\n",
        "    return [\n",
        "        [entity if entity in ['B-PER', 'I-PER'] else 'O' for entity in entities]\n",
        "        for entities in se\n",
        "    ]\n",
        "\n",
        "def remove_miscs(se: list):\n",
        "    return [\n",
        "        [entity if not is_misc(entity) else 'O' for entity in entities]\n",
        "        for entities in se\n",
        "    ]\n",
        "\n",
        "# Load the DaNE data\n",
        "_, _, test = DDT().load_as_simple_ner(predefined_splits=True)\n",
        "sentences_tokens, sentences_entities = test\n",
        "\n",
        "# Replace MISC with O for fair comparisons\n",
        "sentences_entities = remove_miscs(sentences_entities)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEvIsWmLt9sg",
        "outputId": "7ed9c36f-3790-4253-d1c6-106bd034b154"
      },
      "source": [
        "num_sentences = len(sentences_tokens)\n",
        "num_tokens = sum([len(s) for s in sentences_tokens])\n",
        "print(num_sentences, num_tokens)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "565 10023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze-aoGNbk5l0"
      },
      "source": [
        "# Mutate to firstn name to new ethnicity\n",
        "Issue here is that all other ethnicities' names are only female"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTtDJNlJuBCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4d5d6d-6d86-4071-95cc-fb9623a10ee6"
      },
      "source": [
        "danish_accepted_names_2010 = pd.read_excel('data/Godkendte Danske Fornavne.xlsx')\n",
        "print(danish_accepted_names_2010['Etnicity'].unique())\n",
        "print(danish_accepted_names_2010.keys())\n",
        "print(len(danish_accepted_names_2010))\n",
        "# remove firstnames with spaces (middle names)\n",
        "cols = [ ' ' not in c for c in danish_accepted_names_2010['FirstName'] ]\n",
        "danish_accepted_names_2010 = danish_accepted_names_2010[cols] \n",
        "print(len(danish_accepted_names_2010))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dansk' 'muslimsk' 'tamilsk' 'afrikansk' 'burma' nan]\nIndex(['FirstName', 'Sex', 'Etnicity'], dtype='object')\n24560\n24339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHfO3sm1pUY6",
        "outputId": "cd3290e9-20cf-4103-92a6-98b31e535771"
      },
      "source": [
        "# Get all other ethnicity than danish first names\n",
        "other_names = danish_accepted_names_2010[danish_accepted_names_2010['Etnicity'] != 'Dansk']['FirstName']\n",
        "print(other_names.sample().item())\n",
        "\n",
        "danish_names = danish_accepted_names_2010[danish_accepted_names_2010['Etnicity'] == 'Dansk'][danish_accepted_names_2010['Sex'] == 'Female']['FirstName']\n",
        "print(danish_names.sample().item())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nujaym\n",
            "Wiwi\n",
            "<ipython-input-28-835393a60621>:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  danish_names = danish_accepted_names_2010[danish_accepted_names_2010['Etnicity'] == 'Dansk'][danish_accepted_names_2010['Sex'] == 'Female']['FirstName']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swxSxZIQp_fS"
      },
      "source": [
        "def get_nondanish_dataset(tokens):\n",
        "    sentences_tokens_other = copy.deepcopy(tokens)\n",
        "    for i in range(len(sentences_tokens_other)):\n",
        "        for j in range(len(sentences_tokens_other[i])):\n",
        "            if sentences_entities[i][j] == 'B-PER':\n",
        "                sentences_tokens_other[i][j] = other_names.sample().item()\n",
        "    return sentences_tokens_other"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UFYBmwFs2to"
      },
      "source": [
        "def get_randomized_danish_dataset(tokens):\n",
        "    sentences_tokens_danish = copy.deepcopy(tokens)\n",
        "    for i in range(len(sentences_tokens_danish)):\n",
        "        for j in range(len(sentences_tokens_danish[i])):\n",
        "            if sentences_entities[i][j] == 'B-PER':\n",
        "                sentences_tokens_danish[i][j] = danish_names.sample().item()\n",
        "\n",
        "    return sentences_tokens_danish"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9OK-459katH"
      },
      "source": [
        "# Test DaNLP BERT NER model original DaNE test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVuotKigkrUF",
        "outputId": "8be6bb73-c045-41ed-ee31-42c22acd75a5"
      },
      "source": [
        "from danlp.models import load_bert_ner_model\n",
        "bert = load_bert_ner_model()\n",
        "\n",
        "predictions = []\n",
        "for i, sentence in enumerate(sentences_tokens):\n",
        "    _, pred_ents = bert.predict(sentence)\n",
        "    predictions.append(pred_ents)\n",
        "print('BERT:')\n",
        "\n",
        "assert len(predictions) == num_sentences\n",
        "data = f1_report(sentences_entities, remove_miscs(predictions), bio=True)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-212dc463d4e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_ents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_ents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BERT:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/danlp/models/bert_models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text, IOBformat)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtokens_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The Bert ner model can maximum take 512 tokens as input. Split instead you text before calling predict. Eg by using sentence boundary detection'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2126\u001b[0m                 ``convert_tokens_to_ids`` method).\n\u001b[1;32m   2127\u001b[0m         \"\"\"\n\u001b[0;32m-> 2128\u001b[0;31m         encoded_inputs = self.encode_plus(\n\u001b[0m\u001b[1;32m   2129\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2130\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2442\u001b[0m         )\n\u001b[1;32m   2443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2444\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2445\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mbatched_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         batched_output = self._batch_encode_plus(\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0mbatched_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    373\u001b[0m         )\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    376\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsISrmax0870",
        "outputId": "3337ab58-1a54-4696-919c-8bdf13f76435"
      },
      "source": [
        "f1_score(isolate_per(sentences_entities), isolate_per(predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWggWXnGtGRs"
      },
      "source": [
        "## Sanity check (mutating danish to danish female names)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Myksf5XszSW",
        "outputId": "1201ac06-bed8-4928-f240-dfcf97a2beba"
      },
      "source": [
        "from danlp.models import load_bert_ner_model\n",
        "bert = load_bert_ner_model()\n",
        "\n",
        "# Bootstrap results\n",
        "F1_PER = list()\n",
        "for j in range(100):\n",
        "    predictions = []\n",
        "    sentences_tokens_danish = get_randomized_danish_dataset(sentences_tokens)\n",
        "    for i, sentence in enumerate(sentences_tokens_danish):\n",
        "        _, pred_ents = bert.predict(sentence)\n",
        "        predictions.append(pred_ents)\n",
        "\n",
        "    F1_PER.append(f1_score(isolate_per(sentences_entities), isolate_per(predictions)))\n",
        "    print(j, F1_PER[-1])\n",
        "result = np.array(F1_PER)\n",
        "print(result.mean(), result.std())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.9\n",
            "1 0.9035812672176309\n",
            "2 0.8997289972899728\n",
            "3 0.9293478260869564\n",
            "4 0.9150684931506851\n",
            "5 0.9159891598915989\n",
            "6 0.9234972677595629\n",
            "7 0.8937329700272479\n",
            "8 0.8991825613079019\n",
            "9 0.9010989010989011\n",
            "10 0.888283378746594\n",
            "11 0.9056603773584906\n",
            "12 0.9209809264305178\n",
            "13 0.9272237196765499\n",
            "14 0.905149051490515\n",
            "15 0.9016393442622951\n",
            "16 0.9405405405405406\n",
            "17 0.862533692722372\n",
            "18 0.89196675900277\n",
            "19 0.9095890410958904\n",
            "20 0.9293478260869564\n",
            "21 0.9071038251366121\n",
            "22 0.907608695652174\n",
            "23 0.9110512129380054\n",
            "24 0.9100817438692099\n",
            "25 0.9130434782608695\n",
            "26 0.8972972972972973\n",
            "27 0.9021739130434783\n",
            "28 0.9139784946236558\n",
            "29 0.8907103825136612\n",
            "30 0.9130434782608695\n",
            "31 0.8888888888888888\n",
            "32 0.8975069252077562\n",
            "33 0.9046321525885559\n",
            "34 0.9130434782608695\n",
            "35 0.9226519337016573\n",
            "36 0.8980716253443526\n",
            "37 0.9046321525885559\n",
            "38 0.8888888888888887\n",
            "39 0.8937329700272479\n",
            "40 0.8876712328767123\n",
            "41 0.9175824175824175\n",
            "42 0.9002695417789757\n",
            "43 0.8997289972899728\n",
            "44 0.9180327868852459\n",
            "45 0.8956043956043955\n",
            "46 0.8888888888888887\n",
            "47 0.9095890410958904\n",
            "48 0.9268292682926829\n",
            "49 0.9016393442622951\n",
            "50 0.9060773480662984\n",
            "51 0.9021739130434783\n",
            "52 0.9347826086956522\n",
            "53 0.9065934065934065\n",
            "54 0.9260273972602739\n",
            "55 0.8967391304347826\n",
            "56 0.921832884097035\n",
            "57 0.8972972972972973\n",
            "58 0.9135135135135135\n",
            "59 0.9041095890410958\n",
            "60 0.9065934065934065\n",
            "61 0.9155313351498637\n",
            "62 0.8948787061994611\n",
            "63 0.9016393442622951\n",
            "64 0.8931506849315068\n",
            "65 0.889487870619946\n",
            "66 0.9035812672176309\n",
            "67 0.9116022099447513\n",
            "68 0.8997289972899728\n",
            "69 0.9135135135135135\n",
            "70 0.8722222222222223\n",
            "71 0.9150684931506851\n",
            "72 0.924731182795699\n",
            "73 0.9065934065934065\n",
            "74 0.8961748633879781\n",
            "75 0.9100817438692099\n",
            "76 0.8821917808219178\n",
            "77 0.8760330578512396\n",
            "78 0.8956043956043955\n",
            "79 0.9139784946236558\n",
            "80 0.9171270718232044\n",
            "81 0.9164420485175202\n",
            "82 0.9105691056910569\n",
            "83 0.9056603773584906\n",
            "84 0.9095890410958904\n",
            "85 0.9081081081081082\n",
            "86 0.9239130434782609\n",
            "87 0.9056603773584906\n",
            "88 0.9184782608695653\n",
            "89 0.8991825613079019\n",
            "90 0.9239130434782609\n",
            "91 0.8986301369863013\n",
            "92 0.8988764044943819\n",
            "93 0.9010989010989011\n",
            "94 0.9146005509641874\n",
            "95 0.9021739130434783\n",
            "96 0.8888888888888888\n",
            "97 0.8901098901098902\n",
            "98 0.9100817438692099\n",
            "99 0.8852459016393444\n",
            "0.9054800298776097 0.013271710874611332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ5MThHTrcE6"
      },
      "source": [
        "# Test DaNLP BERT NER model mutated other ethnicity first name DaNE test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nYNPDXTk0C3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "650e1abb-3cce-427a-fe22-d5362f36b802"
      },
      "source": [
        "# Bootstrap results\n",
        "F1_PER = list()\n",
        "for j in range(100):\n",
        "    predictions = []\n",
        "    sentences_tokens_other = get_nondanish_dataset(sentences_tokens)\n",
        "    for i, sentence in enumerate(sentences_tokens_other):\n",
        "        _, pred_ents = bert.predict(sentence)\n",
        "        predictions.append(pred_ents)\n",
        "\n",
        "    F1_PER.append(f1_score(isolate_per(sentences_entities), isolate_per(predictions)))\n",
        "    print(j, F1_PER[-1])\n",
        "result = np.array(F1_PER)\n",
        "print(result.mean(), result.std())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.8876712328767123\n",
            "1 0.8594594594594595\n",
            "2 0.905149051490515\n",
            "3 0.9010989010989011\n",
            "4 0.8815426997245178\n",
            "5 0.9065934065934065\n",
            "6 0.8858695652173912\n",
            "7 0.8743169398907102\n",
            "8 0.8956043956043955\n",
            "9 0.8797814207650273\n",
            "10 0.8876712328767123\n",
            "11 0.88283378746594\n",
            "12 0.8695652173913044\n",
            "13 0.8719346049046323\n",
            "14 0.8937329700272479\n",
            "15 0.8991825613079019\n",
            "16 0.891304347826087\n",
            "17 0.8950276243093923\n",
            "18 0.8858695652173912\n",
            "19 0.89196675900277\n",
            "20 0.888283378746594\n",
            "21 0.894308943089431\n",
            "22 0.8870523415977961\n",
            "23 0.8743169398907102\n",
            "24 0.858695652173913\n",
            "25 0.905149051490515\n",
            "26 0.8907103825136612\n",
            "27 0.8702702702702702\n",
            "28 0.883977900552486\n",
            "29 0.858695652173913\n",
            "30 0.883977900552486\n",
            "31 0.9071038251366121\n",
            "32 0.8918918918918918\n",
            "33 0.9105691056910569\n",
            "34 0.8698060941828255\n",
            "35 0.8876712328767123\n",
            "36 0.8648648648648649\n",
            "37 0.8907103825136612\n",
            "38 0.89247311827957\n",
            "39 0.8918918918918918\n",
            "40 0.8780487804878048\n",
            "41 0.9120879120879122\n",
            "42 0.8617886178861788\n",
            "43 0.9032258064516129\n",
            "44 0.8797814207650273\n",
            "45 0.8834688346883468\n",
            "46 0.868131868131868\n",
            "47 0.8967391304347826\n",
            "48 0.8493150684931507\n",
            "49 0.9046321525885559\n",
            "50 0.9016393442622951\n",
            "51 0.883977900552486\n",
            "52 0.8961748633879781\n",
            "53 0.89247311827957\n",
            "54 0.891304347826087\n",
            "55 0.8846153846153847\n",
            "56 0.9056603773584906\n",
            "57 0.9209809264305178\n",
            "58 0.907608695652174\n",
            "59 0.8961748633879781\n",
            "60 0.875\n",
            "61 0.9037433155080213\n",
            "62 0.9071038251366121\n",
            "63 0.8702702702702702\n",
            "64 0.8729281767955802\n",
            "65 0.8423913043478262\n",
            "66 0.905149051490515\n",
            "67 0.8797814207650273\n",
            "68 0.8571428571428572\n",
            "69 0.8808864265927976\n",
            "70 0.8610354223433243\n",
            "71 0.905149051490515\n",
            "72 0.8712328767123287\n",
            "73 0.8994413407821229\n",
            "74 0.8743169398907102\n",
            "75 0.9046321525885559\n",
            "76 0.8967391304347826\n",
            "77 0.8609625668449198\n",
            "78 0.8931506849315068\n",
            "79 0.8791208791208792\n",
            "80 0.9046321525885559\n",
            "81 0.8650137741046832\n",
            "82 0.8980716253443526\n",
            "83 0.894308943089431\n",
            "84 0.8954423592493298\n",
            "85 0.9002695417789757\n",
            "86 0.9041095890410958\n",
            "87 0.8991825613079019\n",
            "88 0.9150684931506851\n",
            "89 0.8888888888888887\n",
            "90 0.9105691056910569\n",
            "91 0.8901098901098902\n",
            "92 0.9110512129380054\n",
            "93 0.8907103825136612\n",
            "94 0.9056603773584906\n",
            "95 0.8937329700272479\n",
            "96 0.9021739130434783\n",
            "97 0.8657534246575342\n",
            "98 0.8760330578512396\n",
            "99 0.8719346049046323\n",
            "0.8878932053802779 0.016023141183567076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVblU-A-ik5T"
      },
      "source": [
        "# Prepare data for DaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrnnMRw7ij9Y"
      },
      "source": [
        "import os\n",
        "import pyconll\n",
        "from danlp.download import DEFAULT_CACHE_DIR, download_dataset, _unzip_process_func, DATASETS\n",
        "dataset_dir = download_dataset('ddt', process_func=_unzip_process_func, cache_dir=DEFAULT_CACHE_DIR)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqLCJBocjSAi"
      },
      "source": [
        "## Modified data class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCx6QTHwi1Yw"
      },
      "source": [
        "class DDT_:\n",
        "    \"\"\"\n",
        "    Class for loading the Danish Dependency Treebank (DDT) through several frameworks/formats.   \n",
        "    The DDT dataset has been annotated with NER tags in the IOB2 format.\n",
        "    The dataset is downloaded in CoNLL-U format, but with this class\n",
        "    it can be converted to spaCy format or a simple NER format\n",
        "    similar to the CoNLL 2003 NER format.\n",
        "    :param str cache_dir: the directory for storing cached models\n",
        "    :param bool verbose: `True` to increase verbosity\n",
        "    \"\"\"\n",
        "    def __init__(self, cache_dir: str = DEFAULT_CACHE_DIR):\n",
        "        self.dataset_name = 'ddt'\n",
        "        self.file_extension = DATASETS[self.dataset_name]['file_extension']\n",
        "        self.dataset_dir = download_dataset('ddt', process_func=_unzip_process_func, cache_dir=cache_dir)\n",
        "\n",
        "    def load_as_conllu(self, predefined_splits: bool = False):\n",
        "        \"\"\"\n",
        "        Load the DDT in CoNLL-U format.Œ\n",
        "        :param bool predefined_splits:\n",
        "        :return: A single pyconll.Conll\n",
        "                or a tuple of (train, dev, test) pyconll.Conll\n",
        "                depending on predefined_split\n",
        "        \"\"\"\n",
        "\n",
        "        parts = [None, None, None]  # Placeholder list to put predefined parts of dataset [train, dev, test]\n",
        "        for i, part in enumerate(['train', 'dev', 'test']):\n",
        "            file_name = \"{}.{}{}\".format(self.dataset_name, part, self.file_extension)\n",
        "            file_path = os.path.join(self.dataset_dir, file_name)\n",
        "\n",
        "            parts[i] = pyconll.load_from_file(file_path)\n",
        "\n",
        "        # if predefined_splits: then we should return three files\n",
        "        if predefined_splits:\n",
        "            return parts\n",
        "\n",
        "        # Merge the splits to one single dataset\n",
        "        parts[0].extend(parts[1])\n",
        "        parts[0].extend(parts[2])\n",
        "\n",
        "        return parts[0]\n",
        "\n",
        "    def load_as_simple_ner(self, predefined_splits: bool = False):\n",
        "        conllu_parts = self.load_as_conllu(predefined_splits)\n",
        "\n",
        "        if not predefined_splits:\n",
        "            conllu_parts = [conllu_parts]\n",
        "\n",
        "        parts = []\n",
        "        for conllu_part in conllu_parts:\n",
        "            full_sentences = []\n",
        "            part_sentences = []\n",
        "            part_entities = []\n",
        "\n",
        "            for sent in conllu_part:\n",
        "                part_sentences.append([token.form for token in sent._tokens])\n",
        "                part_entities.append([token.misc['name'].pop() for token in sent._tokens])\n",
        "                full_sentences.append(sent)\n",
        "\n",
        "            parts.append([part_sentences, part_entities, full_sentences])\n",
        "\n",
        "        if predefined_splits:\n",
        "            return parts\n",
        "        return parts[0]\n",
        "\n",
        "\n",
        "    def load_with_flair(self, predefined_splits: bool = False):\n",
        "        \"\"\"\n",
        "        Load the DDT with flair. \n",
        "        This function is inspired by the \"Reading Your Own Sequence Labeling Dataset\" from Flairs tutorial\n",
        "        on reading corpora:\n",
        "        https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md\n",
        "        :param predefined_splits:\n",
        "        :type predefined_splits: bool\n",
        "        :return: ColumnCorpus\n",
        "        .. note:: TODO: Make a pull request to flair similar to this:\n",
        "            https://github.com/zalandoresearch/flair/issues/383\n",
        "        \"\"\"\n",
        "\n",
        "        from flair.data import Corpus\n",
        "        from flair.datasets import ColumnCorpus\n",
        "\n",
        "        columns = {1: 'text', 3: 'pos', 9: 'ner'}\n",
        "\n",
        "        # init a corpus using column format, data folder and the names of the train, dev and test files\n",
        "        corpus: Corpus = ColumnCorpus(self.dataset_dir, columns, comment_symbol='#',\n",
        "                                      train_file='{}.{}{}'.format(self.dataset_name, 'train', self.file_extension),\n",
        "                                      test_file='{}.{}{}'.format(self.dataset_name, 'test', self.file_extension),\n",
        "                                      dev_file='{}.{}{}'.format(self.dataset_name, 'dev', self.file_extension))\n",
        "\n",
        "        # Remove the `name=` from `name=B-PER` to only use the `B-PER` tag\n",
        "        parts = ['train', 'dev', 'test']\n",
        "        for part in parts:\n",
        "            dataset = corpus.__getattribute__(part)\n",
        "\n",
        "            for sentence in dataset.sentences:\n",
        "                for token in sentence.tokens:\n",
        "                    if 'ner' in token.tags:\n",
        "                        token.tags['ner'].value = token.tags['ner'].value.split(\"=\")[1].replace(\"|SpaceAfter\", \"\")\n",
        "\n",
        "        return corpus\n",
        "\n",
        "    def load_with_spacy(self):\n",
        "        \"\"\"\n",
        "        Loads the DDT with spaCy. \n",
        "        \n",
        "        This function converts the conllu files to json in the spaCy format.\n",
        "        :return: GoldCorpus\n",
        "        .. note:: Not using jsonl because of:\n",
        "            https://github.com/explosion/spaCy/issues/3523\n",
        "        \"\"\"\n",
        "        import srsly\n",
        "        from spacy.cli.converters import conllu2json\n",
        "        from spacy.gold import GoldCorpus\n",
        "        from spacy.gold import Path\n",
        "\n",
        "        for part in ['train', 'dev', 'test']:\n",
        "            conll_path = os.path.join(self.dataset_dir, '{}.{}{}'.format(self.dataset_name, part, self.file_extension))\n",
        "            json_path = os.path.join(self.dataset_dir, \"ddt.{}.json\".format(part))\n",
        "\n",
        "            if not os.path.isfile(json_path):  # Convert the conllu files to json\n",
        "                with open(conll_path, 'r') as file:\n",
        "                    file_as_string = file.read()\n",
        "                    file_as_string = file_as_string.replace(\"name=\", \"\").replace(\"|SpaceAfter=No\", \"\")\n",
        "                    file_as_json = conllu2json(file_as_string)\n",
        "\n",
        "                    srsly.write_json(json_path, file_as_json)\n",
        "\n",
        "        train_json_path = os.path.join(self.dataset_dir, \"ddt.train.json\")\n",
        "        dev_json_path = os.path.join(self.dataset_dir, \"ddt.dev.json\")\n",
        "\n",
        "        assert os.path.isfile(train_json_path)\n",
        "        assert os.path.isfile(dev_json_path)\n",
        "\n",
        "        return GoldCorpus(Path(train_json_path), Path(dev_json_path))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5ZPn5vJjVlc"
      },
      "source": [
        "## Load data with modified class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bKFqJnIyjZK9",
        "outputId": "2bb3dd07-f3f5-4ac1-9ab0-d35056d2dc97"
      },
      "source": [
        "\n",
        "# Load the DaNE data\n",
        "_, _, test = DDT_().load_as_simple_ner(predefined_splits=True)\n",
        "sentences_tokens, sentences_entities, sentences = test\n",
        "\n",
        "# Replace MISC with O for fair comparisons\n",
        "sentences_entities = remove_miscs(sentences_entities)\n",
        "sentences[0].text"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'To kendte russiske historikere Andronik Mirganjan og Igor Klamkin tror ikke, at Rusland kan udvikles uden en \"jernnæve\".'"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUEd9eiBHxM_"
      },
      "source": [
        "# Test DaCy original data + sanity check danish dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77bJfXBsroGr"
      },
      "source": [
        "import dacy\n",
        "import spacy\n",
        "spacy.prefer_gpu()\n",
        "ner_model = dacy.load(\"da_dacy_large_tft-0.0.0\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0da7cb975b245d9e6574458c7c89dfd9?download: 1.82GB [11:26, 2.65MB/s]                            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95MVDYHjnJNA"
      },
      "source": [
        "ner_model.pipeline\n",
        "from spacy.tokens import Doc\n",
        "doctest = Doc(ner_model.vocab, words=sentences_tokens[0])\n",
        "doctest_out = doctest\n",
        "for pipe in ner_model.pipeline:\n",
        "    doctest_out = pipe[1](doctest_out)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWzfHI1Nf8zk",
        "outputId": "0f7b1629-88d0-4bbb-9f1e-fd8e132dcdf0"
      },
      "source": [
        "def isolate_per_(se: list):\n",
        "    return [\n",
        "        ['PER' if entity in ['B-PER', 'I-PER'] else 'O' for entity in entities]\n",
        "        for entities in se\n",
        "    ]\n",
        "predictions = []\n",
        "\n",
        "for i, words in enumerate(sentences_tokens):\n",
        "    doc = Doc(ner_model.vocab, words=words)\n",
        "    for pipe in ner_model.pipeline:\n",
        "        doc = pipe[1](doc)\n",
        "    pred = [ x.ent_type_ if 'PER' in x.ent_type_  else 'O' for x in doc ]\n",
        "    if len(sentences_entities[i]) == len(pred):        \n",
        "        predictions.append(pred)\n",
        "    else:\n",
        "        print(sentences_entities[i], pred, sentences[i].text) \n",
        "    \n",
        "print(f1_score(isolate_per_(sentences_entities), predictions))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9415041782729804\n",
            "/Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PER seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "565"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "len(sentences_entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "565"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "len(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx, i in enumerate(zip(sentences_entities, predictions)):\n",
        "    if len(i[0]) != len(i[1]):\n",
        "        print(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72\n158\n237\n241\n253\n259\n260\n307\n313\n316\n319\n320\n387\n418\n423\n475\n"
          ]
        }
      ],
      "source": [
        "wrong_pred = []\n",
        "for idx, i in enumerate(zip(isolate_per_(sentences_entities), predictions)):\n",
        "    for y, y_hat in zip(i[0], i[1]):\n",
        "        if y != y_hat:\n",
        "            print(idx)\n",
        "            wrong_pred.append(idx)\n",
        "            break\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----\nHer morer Super-Pedersen sig over Nettos priser\n[('O', 'O'), ('O', 'O'), ('O', 'B-PER'), ('O', 'O'), ('O', 'O'), ('O', 'B-ORG'), ('O', 'O')]\n----\nPsykedelisk-mønstret klokkehat 149 kr. og Marilyn Monroe badedragt i kulørt Lycra med Marilyn fotografier på numsen 299 kr. - som også er prisen for mange af husets andre badedragter .\n[('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('PER', 'B-PER'), ('PER', 'I-PER'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('PER', 'B-PER'), ('PER', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O')]\n----\nAlcala havde punktering efter fem kilometer , og Greg Lemond punkterede efter kun 10 meters kørsel .\n[('O', 'B-PER'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('PER', 'B-PER'), ('PER', 'I-PER'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O')]\n----\nHer gled Rolf Sørensens baghjul , så han styrtede og rev både Moreno Argentin og Cassani med .\n[('O', 'O'), ('O', 'O'), ('PER', 'B-PER'), ('PER', 'I-PER'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'B-PER'), ('O', 'I-PER'), ('O', 'O'), ('PER', 'B-PER'), ('O', 'O'), ('O', 'O')]\n----\nHan har været med til over for DSB at fastslå , at både Ask og Urd var gode skibe .\n[('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'B-ORG'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('PER', 'B-ORG'), ('O', 'O'), ('PER', 'B-ORG'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O')]\n----\nDen ene maskine brød sammen , hvorefter Urd vendte om og humpede til Århus , fordi DSB her har reservedele .\n[('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('PER', 'B-ORG'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'B-LOC'), ('O', 'O'), ('O', 'O'), ('O', 'B-ORG'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O')]\n----\nImens bankede Ask i kajen i Kalundborg , hvorefter enhver kunne se , at nu skulle Urd i rute som Ask .\n[('O', 'O'), ('O', 'O'), ('PER', 'B-ORG'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'B-LOC'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('PER', 'B-ORG'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('PER', 'B-ORG'), ('O', 'O')]\n----\nSiden Ungbo for tre uger siden gik konkurs med et underskud på 44,1 mill. kroner , er der næppe gået en dag uden nye belastende afsløringer af Ungbos måde at drive almennyttig udlejningsvirksomhed på er dukket op .\n[('O', 'O'), ('O', 'B-ORG'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('PER', 'B-ORG'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O')]\n----\nDet er en dum telefon , \" sagde \" Mercedesmanden \" .\n[('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'B-PER'), ('O', 'O'), ('O', 'O')]\n----\nComputermanden : \" Hvorfor skulle den det ?\n[('O', 'B-PER'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O')]\n----\n\" Mercedes-manden \" : \" Jeg skulle gå til en anden telefon , fordi han skulle fortælle om sine udskejelser i Holland . \"\n[('O', 'O'), ('O', 'B-PER'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'B-LOC'), ('O', 'O'), ('O', 'O')]\n----\nInden computermanden kørte mod Holland , lyttede politiet også på en samtale fra \" Mercedes-manden \" til én af hans venner .\n[('O', 'O'), ('O', 'B-PER'), ('O', 'O'), ('O', 'O'), ('O', 'B-LOC'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'B-PER'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O')]\n----\nPalle Kreiberg var lærer , men i sin fritid arbejdede han som luftfotograf med firmanavnet Gy-Kort .\n[('PER', 'B-PER'), ('PER', 'I-PER'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('PER', 'B-ORG'), ('O', 'O')]\n----\nDet samme vil Bank .\n[('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'B-PER'), ('O', 'O')]\n----\nGENBRUG\n[('PER', 'O')]\n----\nDepeche har det selv morsomt mens de indspiller pladen , hvor Martin f.eks. går på \" Jungle Club \" og ser på mandestrip , men bortset fra en samplet optagelse af Dave der drejer tændingsnøglen i sin nyindkøbte Porsche , er der ikke meget at grine af på \" Black Celebration \" .\n[('O', 'B-PER'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('PER', 'B-PER'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'B-LOC'), ('O', 'I-LOC'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('PER', 'B-PER'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O')]\n"
          ]
        }
      ],
      "source": [
        "for idx in wrong_pred:\n",
        "    print(\"----\")\n",
        "    print(\" \".join(sentences_tokens[idx]))\n",
        "    print(list(zip(predictions[idx], sentences_entities[idx])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Depeche\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n har det selv morsomt mens de indspiller pladen , hvor \n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Martin\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n</mark>\n f.eks. går på &quot; \n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Jungle Club\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n</mark>\n &quot; og ser på mandestrip , men bortset fra en samplet optagelse af \n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Dave\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n</mark>\n der drejer tændingsnøglen i sin nyindkøbte \n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Porsche\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n</mark>\n , er der ikke meget at grine af på &quot; \n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Black Celebration\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n</mark>\n&quot; </div></span>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "doc = ner_model(\"\"\"Depeche har det selv morsomt mens de indspiller pladen , hvor Martin f.eks. går på \" Jungle Club \" og ser på mandestrip , men bortset fra en samplet optagelse af Dave der drejer tændingsnøglen i sin nyindkøbte Porsche , er der ikke meget at grine af på \" Black Celebration\" \"\"\")\n",
        "displacy.render(doc, style=\"ent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JYrNJsLFCGO"
      },
      "source": [
        "## Sanity check (mutating danish to other ethnicity female names)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-VnZ_kmj_rf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5fbd67d-8575-409a-a9e1-6e1b9a1977b5"
      },
      "source": [
        "# Bootstrap results\n",
        "F1_PER = list()\n",
        "for j in range(100):\n",
        "    predictions = []\n",
        "    sentences_tokens_other = get_nondanish_dataset(sentences_tokens)\n",
        "    for i, sentence in enumerate(sentences_tokens_other):\n",
        "        doc = Doc(ner_model.vocab, words=sentence)\n",
        "        for pipe in ner_model.pipeline:\n",
        "            doc = pipe[1](doc)\n",
        "        pred = [ x.ent_type_ if 'PER' in x.ent_type_  else 'O' for x in doc ]\n",
        "        predictions.append(pred)\n",
        "\n",
        "    F1_PER.append(f1_score(isolate_per_(sentences_entities), predictions))\n",
        "    print(j, F1_PER[-1])\n",
        "result = np.array(F1_PER)\n",
        "print(result.mean(), result.std())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.9352112676056339\n",
            "1 0.9415041782729804\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-8691f41f704a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mner_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ment_type_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'PER'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ment_type_\u001b[0m  \u001b[0;32melse\u001b[0m \u001b[0;34m'O'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/spacy_transformers/pipeline_component.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mnightly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \"\"\"\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/spacy_transformers/pipeline_component.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullTransformerBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mbatch_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerListener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlistener\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisteners\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0monly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \"\"\"\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/spacy_transformers/layers/transformer_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, docs, is_train)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mwordpieces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_max_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordpieces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"logger\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mlog_gpu_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logger\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"after forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    287\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/thinc/layers/pytorchwrapper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mXtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mYtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_backprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dYtorch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtorch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/thinc/shims/pytorch.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, is_train)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArgsKwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/thinc/shims/pytorch.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         )\n\u001b[0;32m--> 709\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    710\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    442\u001b[0m                 )\n\u001b[1;32m    443\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    445\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add cross attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         )\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1782\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1784\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/NLP/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f72B9pbgFEQI"
      },
      "source": [
        "## Sanity check (mutating danish to danish female names)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHcq7aPnvQ-p",
        "outputId": "5de46391-0b31-425d-c2f7-2447975983df"
      },
      "source": [
        "# Bootstrap results\n",
        "F1_PER = list()\n",
        "for j in range(100):\n",
        "    predictions = []\n",
        "    sentences_tokens_danish = get_randomized_danish_dataset(sentences_tokens)\n",
        "    for i, sentence in enumerate(sentences_tokens_danish):\n",
        "        doc = Doc(ner_model.vocab, words=sentence)\n",
        "        for pipe in ner_model.pipeline:\n",
        "            doc = pipe[1](doc)\n",
        "        pred = [ x.ent_type_ if 'PER' in x.ent_type_  else 'O' for x in doc ]\n",
        "        predictions.append(pred)\n",
        "\n",
        "    F1_PER.append(f1_score(isolate_per_(sentences_entities), predictions))\n",
        "    print(j, F1_PER[-1])\n",
        "result = np.array(F1_PER)\n",
        "print(result.mean(), result.std())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PER seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 0.9444444444444444\n",
            "1 0.9589041095890412\n",
            "2 0.9529085872576178\n",
            "3 0.9444444444444444\n",
            "4 0.9441340782122906\n",
            "5 0.9415041782729804\n",
            "6 0.9502762430939227\n",
            "7 0.9441340782122906\n",
            "8 0.9473684210526315\n",
            "9 0.9415041782729804\n",
            "10 0.956043956043956\n",
            "11 0.9382022471910112\n",
            "12 0.953168044077135\n",
            "13 0.9295774647887323\n",
            "14 0.9500000000000001\n",
            "15 0.9500000000000001\n",
            "16 0.9441340782122906\n",
            "17 0.9411764705882353\n",
            "18 0.9325842696629215\n",
            "19 0.9418282548476455\n",
            "20 0.9444444444444444\n",
            "21 0.9502762430939227\n",
            "22 0.9502762430939227\n",
            "23 0.9355742296918768\n",
            "24 0.9388888888888889\n",
            "25 0.9385474860335196\n",
            "26 0.9441340782122906\n",
            "27 0.9329608938547486\n",
            "28 0.9291784702549574\n",
            "29 0.9299719887955182\n",
            "30 0.9500000000000001\n",
            "31 0.9352112676056339\n",
            "32 0.9415041782729804\n",
            "33 0.9444444444444444\n",
            "34 0.9500000000000001\n",
            "35 0.9385474860335196\n",
            "36 0.935933147632312\n",
            "37 0.947075208913649\n",
            "38 0.9473684210526315\n",
            "39 0.9529085872576178\n",
            "40 0.9558011049723757\n",
            "41 0.9473684210526315\n",
            "42 0.9269662921348315\n",
            "43 0.9411764705882353\n",
            "44 0.9586776859504132\n",
            "45 0.9500000000000001\n",
            "46 0.9411764705882353\n",
            "47 0.9529085872576178\n",
            "48 0.9586776859504132\n",
            "49 0.9441340782122906\n",
            "50 0.923943661971831\n",
            "51 0.9388888888888889\n",
            "52 0.9441340782122906\n",
            "53 0.9444444444444444\n",
            "54 0.9529085872576178\n",
            "55 0.9352112676056339\n",
            "56 0.9385474860335196\n",
            "57 0.9476584022038568\n",
            "58 0.9385474860335196\n",
            "59 0.9173789173789174\n",
            "60 0.9418282548476455\n",
            "61 0.9441340782122906\n",
            "62 0.9444444444444444\n",
            "63 0.9265536723163842\n",
            "64 0.9411764705882353\n",
            "65 0.953168044077135\n",
            "66 0.9444444444444444\n",
            "67 0.9382022471910112\n",
            "68 0.9441340782122906\n",
            "69 0.9415041782729804\n",
            "70 0.9352112676056339\n",
            "71 0.9473684210526315\n",
            "72 0.9265536723163842\n",
            "73 0.935933147632312\n",
            "74 0.9500000000000001\n",
            "75 0.9325842696629215\n",
            "76 0.9476584022038568\n",
            "77 0.9329608938547486\n",
            "78 0.9295774647887323\n",
            "79 0.935933147632312\n",
            "80 0.9269662921348315\n",
            "81 0.9500000000000001\n",
            "82 0.9500000000000001\n",
            "83 0.935933147632312\n",
            "84 0.9355742296918768\n",
            "85 0.9444444444444444\n",
            "86 0.9502762430939227\n",
            "87 0.9500000000000001\n",
            "88 0.9447513812154695\n",
            "89 0.9183098591549296\n",
            "90 0.9385474860335196\n",
            "91 0.9299719887955182\n",
            "92 0.9303621169916434\n",
            "93 0.9500000000000001\n",
            "94 0.9558011049723757\n",
            "95 0.9329608938547486\n",
            "96 0.9502762430939227\n",
            "97 0.9385474860335196\n",
            "98 0.9502762430939227\n",
            "99 0.9388888888888889\n",
            "0.9421739192106123 0.00879466304399809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw85Z2Up4r-P"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}