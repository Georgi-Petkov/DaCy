{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('dacy': virtualenvwrapper)"
  },
  "interpreter": {
   "hash": "3306719ab0e316992d1a1835936bef6bb94c1674b49ab3d645e902d91649e7a1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Augmentation\n",
    "This notebook recreates Table X from the paper XX and illustrates how to use the augmenters and scoring functions included in DaCy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^should be true if not: \n",
    "#!pip install spacy[cuda102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # assuming we are located in dacy github repo\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install dacy if not already installed\n",
    "#!pip install -r requirements.txt # assumed version 1.0.0 of dacy\n",
    "# or using\n",
    "#!pip install dacy\n",
    "\n",
    "# download relevant spacy models\n",
    "#!python -m spacy download da_core_news_sm\n",
    "#!python -m spacy download da_core_news_md\n",
    "#!python -m spacy download da_core_news_lg\n",
    "\n",
    "# download danlp dependencies\n",
    "#!pip install danlp==0.0.11 \n",
    "#!pip install transformers==3.5.1 --no-deps # for DaNLP\n",
    "#!pip install gensim==3.8.1 # also danlp\n",
    "#!pip install NERDA\n",
    "#!pip install spacy-stanza\n",
    "#!pip install flair==0.4.5\n",
    "#!pip install torch==1.7.1 # for flair\n",
    "\n",
    "#!pip install polyglot # you will need to install polyglot dependencies as well\n",
    "#!polyglot download pos2.da"
   ]
  },
  {
   "source": [
    "# The dataset: DaNE\n",
    "Start off by loading the test set of the DaNE dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dacy.datasets import dane\n",
    "test = dane(splits=[\"test\"])"
   ]
  },
  {
   "source": [
    "# Augmenters\n",
    "\n",
    "Create a list of augmenters we wish to apply to our model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training.augment import create_lower_casing_augmenter, dont_augment\n",
    "from dacy.augmenters import create_pers_augmenter, create_keyboard_augmenter, create_æøå_augmenter, create_spacing_augmenter\n",
    "from dacy.datasets import danish_names, muslim_names, female_names, male_names\n",
    "\n",
    "# randomly augment names\n",
    "dk_name_dict = danish_names()\n",
    "muslim_name_dict = muslim_names()\n",
    "f_name_dict = female_names()\n",
    "m_name_dict = male_names()\n",
    "\n",
    "dk_aug = create_pers_augmenter(dk_name_dict, force_pattern_size=True, keep_name=False, patterns = [\"fn\", \"fn,ln\", \"fn,ln,ln\"])\n",
    "muslim_aug = create_pers_augmenter(muslim_name_dict, force_pattern_size=True, keep_name=False,  patterns = [\"fn\", \"fn,ln\", \"fn,ln,ln\"])\n",
    "f_aug = create_pers_augmenter(dk_name_dict, force_pattern_size=True, keep_name=False,  patterns = [\"fn\", \"fn,ln\", \"fn,ln,ln\"])\n",
    "m_aug = create_pers_augmenter(muslim_name_dict, force_pattern_size=True, keep_name=False,  patterns = [\"fn\", \"fn,ln\", \"fn,ln,ln\"])\n",
    "punct_aug = create_pers_augmenter(muslim_name_dict, force_pattern_size=False, keep_name=True, patterns = [\"abbpunct\"])\n",
    "\n",
    "\n",
    "# randomly change 5%/15% of characters to a neighbouring key\n",
    "keyboard_aug_02 = create_keyboard_augmenter(doc_level=1, char_level=0.02, keyboard=\"QWERTY_DA\")\n",
    "keyboard_aug_05 = create_keyboard_augmenter(doc_level=1, char_level=0.05, keyboard=\"QWERTY_DA\")\n",
    "keyboard_aug_15 = create_keyboard_augmenter(doc_level=1, char_level=0.15, keyboard=\"QWERTY_DA\")\n",
    "\n",
    "# Change æ=ae, ø=oe, å=aa\n",
    "æøå_aug = create_æøå_augmenter(doc_level=1, char_level=1)\n",
    "\n",
    "# lower case text\n",
    "lower_case_aug = create_lower_casing_augmenter(level=1)\n",
    "\n",
    "# spacing\n",
    "spacing_aug_05 = create_spacing_augmenter(doc_level=1, spacing_level=0.05)\n",
    "spacing_aug = create_spacing_augmenter(doc_level=1, spacing_level=1)\n",
    "\n",
    "n = 20\n",
    "               # augmenter   name               n rep\n",
    "augmenters = [(dont_augment, \"No augmentation\", 1),\n",
    "              (keyboard_aug_02, \"Keystroke errors 2%\", n),\n",
    "              (keyboard_aug_05, \"Keystroke errors 5%\", n), \n",
    "              (keyboard_aug_15, \"Keystroke errors 15%\", n), \n",
    "              (æøå_aug, \"Æøå Augmentation\",  1), \n",
    "              (lower_case_aug, \"Lowercase\" ,1), \n",
    "              (dk_aug, \"Danish names\", n), \n",
    "              (muslim_aug, \"Muslim names\", n),\n",
    "              (f_aug, \"Female names\", n),\n",
    "              (m_aug, \"Male names\", n),\n",
    "              (punct_aug, \"Abbreviated first names\", 1),\n",
    "              (spacing_aug_05, \"Spacing Augmention 5%\", n),\n",
    "              (spacing_aug, \"No Spacing\", 1)\n",
    "              ]\n"
   ]
  },
  {
   "source": [
    "# Apply functions\n",
    "Loading application functions for necessary models. No need to create one for SpaCy pipelines."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-30 18:15:17,670 loading file /home/kenneth/.danlp/flair.ner.pt\n",
      "2021-06-30 18:15:22,026 loading file /home/kenneth/.danlp/flair.pos.pt\n",
      "Device automatically set to: cuda\n",
      "\n",
      "        Model loaded. Please make sure, that you're running the latest version \n",
      "        of 'NERDA' otherwise the model is not guaranteed to work.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from dev.robustness_apply_fn.apply_fn_danlp import apply_danlp_bert\n",
    "from dev.robustness_apply_fn.apply_fn_flair import apply_flair\n",
    "#from dev.robustness_apply_fn.apply_fn_polyglot import apply_polyglot\n",
    "from dev.robustness_apply_fn.apply_fn_nerda import apply_nerda\n"
   ]
  },
  {
   "source": [
    "# Models\n",
    "A list of models to apply. To save memory the models are only loaded in one at a time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"spacy_small\" : \"da_core_news_sm\",\n",
    "    \"spacy_medium\": \"da_core_news_md\",\n",
    "    \"spacy_large\" : \"da_core_news_lg\",\n",
    "    \"dacy_small\" : \"da_dacy_small_tft-0.0.0\",\n",
    "    \"dacy_medium\" : \"da_dacy_medium_tft-0.0.0\",\n",
    "    \"dacy_large\" : \"da_dacy_large_tft-0.0.0\",\n",
    "    \"stanza\": \"da\",\n",
    "    \"flair\" : apply_flair,\n",
    "    #\"polyglot\" : apply_polyglot,\n",
    "    \"danlp_bert\" : apply_danlp_bert,\n",
    "    \"nerda_bert\" : apply_nerda,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download the danlp and nerda you will have to set up a certificate:\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "source": [
    "# Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"robustness\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO]: Scoring model 'spacy_small' using DaCy\n",
      "\t Running augmenter: No augmentation\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29339/818899231.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\t Running augmenter: {nam}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mscores_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapply_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmenters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mscores_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mscores_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"augmenter\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/DaCy/dacy/score/score.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(corpus, apply_fn, score_fn, augmenters, k, nlp, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mscores_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mscores_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/DaCy/dacy/score/score.py\u001b[0m in \u001b[0;36m__score\u001b[0;34m(augmenter)\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscore_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "import dacy\n",
    "from dacy.score import score, n_sents_score\n",
    "\n",
    "\n",
    "for mdl in model_dict:\n",
    "    print(f\"[INFO]: Scoring model '{mdl}' using DaCy\")\n",
    "\n",
    "    # load model\n",
    "    if \"dacy\" in mdl:\n",
    "        apply_fn = dacy.load(model_dict[mdl])\n",
    "    elif \"spacy\" in mdl:\n",
    "        apply_fn = spacy.load(model_dict[mdl])\n",
    "    elif \"stanza\" in mdl:\n",
    "        stanza.download(model_dict[mdl])\n",
    "        # Initialize the pipeline\n",
    "        nlp = spacy_stanza.load_pipeline(model_dict[mdl])\n",
    "    else:\n",
    "        apply_fn = model_dict[mdl]\n",
    "\n",
    "    i = 0\n",
    "    scores = []\n",
    "    for aug, nam, k in augmenters:\n",
    "        print(f\"\\t Running augmenter: {nam}\")\n",
    "\n",
    "        scores_ = score(corpus=test, apply_fn=apply_fn, augmenters=aug, k=k)\n",
    "        scores_[\"model\"] = mdl\n",
    "        scores_[\"augmenter\"] = nam\n",
    "        scores_[\"i\"] = i\n",
    "        scores.append(scores_)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    for n in [5, 10]:\n",
    "        scores_ = n_sents_score(n_sents = n, apply_fn=apply_fn)\n",
    "        scores_[\"model\"] = mdl\n",
    "        scores_[\"augmenter\"] = f\"Input size augmentation {n} sentences\"\n",
    "        scores_[\"i\"] = i + 1\n",
    "        scores.append(scores_)\n",
    "    scores = pd.concat(scores)\n",
    "\n",
    "    scores.to_csv(f\"robustness/{mdl}_augmentation_performance_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}