{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to augmenters\n",
    "\n",
    "This notebook provides a short introduction to some of the tools for augmentation included in `DaCy`. For information on how to  conduct robustness test of your models please see `dacy-robustness.ipynb`.\n",
    "\n",
    "Let's start out by seeing how different augmenters change your text. The augmenters included in `DaCy` work on the `Example` class from SpaCy, so let's write a little helper function that converts a `Doc` to an `Example` and write some text to test on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "from typing import List, Callable, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_example(doc):\n",
    "    return Example(doc, doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "doc = nlp(\"Peter Schmeichel mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod England.\")\n",
    "example = doc_to_example(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how some of the simple augmenters transform the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training.augment import create_lower_casing_augmenter\n",
    "from dacy.augmenters import (create_keyboard_augmenter, create_pers_augmenter,\n",
    "                             create_spacing_augmenter, create_æøå_augmenter)\n",
    "from dacy.datasets import danish_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_aug = create_lower_casing_augmenter(level=1)\n",
    "keyboard_05 = create_keyboard_augmenter(doc_level=1, char_level=0.05, keyboard = \"QWERTY_DA\")\n",
    "keyboard_15 = create_keyboard_augmenter(doc_level=1, char_level=0.15, keyboard = \"QWERTY_DA\")\n",
    "space_aug = create_spacing_augmenter(doc_level=1, spacing_level=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lower_aug` will change all text to lowercase, `keyboard_05` and `keyboard_15` will change 5% or 15% of all characters to a character on a neighbouring key on a Danish QWERTY keyboard (replace `DA` with `EN` for English), and `space_aug` will remove 20% of all whitespaces. The augmenters modify both the reference and the predicted `Doc`s in the Example and makes sure that spans for NER, POS etc. remain correct. Let's see how the text looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = next(lower_aug(nlp, example))\n",
    "key_05 = next(keyboard_05(nlp, example))\n",
    "key_15 = next(keyboard_15(nlp, example))\n",
    "space = next(space_aug(nlp, example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peter schmeichel mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod england.\n",
      "Perer Schmeichel mener også, at det danzke landshold anno 2021 tilhører verdenstoppej og kan vimde den kommende kamp mod Englandæ\n",
      "Pe5er Sfymeicheæ menee også, at det dajske landsgolx qjno 2+21 tilh-rer verxenstopæen og ksn vinde den kommende ma,p mod Wngland.\n",
      "PeterSchmeichel mener også, atdet danskelandshold anno 2021tilhørerverdenstoppen og kanvinde den kommende kamp modEngland.\n"
     ]
    }
   ],
   "source": [
    "for text in [lower.y.text, key_05.y.text, key_15.y.text, space.y.text]:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty neat, right? \n",
    "`DaCy` also includes a more sophisticated augmenter for augmenting names. `create_pers_augmenter` is highly flexible, and can augment names to fit a certain pattern (e.g. first_name, last_name; abbreviated_first_name, last_name) or replace names with one sampled from a dictionary. `DaCy` provides four utility functions for constructing such name dictionaries: `danish_names`, `female_names`, `male_names`, and `muslim_names` (see the README in `datasets/lookup_tables` for sources). The dictionaries are composed of the keys `first_name` and `last_name` which each contain a list of names to sample from. The `pers_augmenter` uses this dictionary when it replaces names to respect first and last names. Let's go through a couple of examples to demonstrate how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['first_name', 'last_name'])\n",
      "['Marie', 'Anna', 'Margrethe', 'Karen', 'Kirstine']\n",
      "['Jensen', 'Nielsen', 'Hansen', 'Pedersen', 'Andersen']\n"
     ]
    }
   ],
   "source": [
    "print(danish_names().keys())\n",
    "print(danish_names()[\"first_name\"][0:5])\n",
    "print(danish_names()[\"last_name\"][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eilif Stengaard var en dansk digter og forfatter\n",
      "1, 2, 3, Skjold Boserup er en mur\n",
      "Susanne Samuelsen mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod England.\n"
     ]
    }
   ],
   "source": [
    "def augment_texts(texts: List[str], augmenter: Callable) -> Iterator[Example]:\n",
    "    \"\"\"Takes a list of strings and yields augmented examples\"\"\"\n",
    "    docs = nlp.pipe(texts)\n",
    "    for doc in docs:\n",
    "        ex = Example(doc, doc)\n",
    "        aug = augmenter(nlp, ex)\n",
    "        yield aug\n",
    "\n",
    "texts = [\n",
    "    \"Hans Christian Andersen var en dansk digter og forfatter\",\n",
    "    \"1, 2, 3, Schmeichel er en mur\",\n",
    "    \"Peter Schmeichel mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod England.\"\n",
    "    ]\n",
    "\n",
    "# Create a dictionary to use for name replacement\n",
    "dk_name_dict = danish_names()\n",
    "\n",
    "\n",
    "# force_pattern augments PER entities to fit the format and length of `patterns`. Patterns allows you to specificy arbitrary\n",
    "# combinations of \"fn\" (first names), \"ln\" (last names), \"abb\" (abbreviated to first character) and \"abbpunct\" (abbreviated\n",
    "# to first character + \".\") separeated by \",\". If keep_name=True, the augmenter will not change names, but if force_pattern_size\n",
    "# is True it will make them fit the length and potentially abbreviate names. \n",
    "pers_aug = create_pers_augmenter(dk_name_dict, force_pattern_size=True, keep_name=False, patterns=[\"fn,ln\"])\n",
    "augmented_docs = augment_texts(texts, pers_aug)\n",
    "for d in augmented_docs:\n",
    "    print(next(d).y.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H. Christian Andersen var en dansk digter og forfatter\n",
      "1, 2, 3, S. er en mur\n",
      "P. Schmeichel mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod England.\n"
     ]
    }
   ],
   "source": [
    "# Here's an example with keep_name=True and force_pattern_size=False which simply abbreviates first names\n",
    "abb_aug = create_pers_augmenter(dk_name_dict, force_pattern_size=False, keep_name=True, patterns=[\"abbpunct\"])\n",
    "augmented_docs = augment_texts(texts, abb_aug)\n",
    "for d in augmented_docs:\n",
    "    print(next(d).y.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M. Brun var en dansk digter og forfatter\n",
      "1, 2, 3, Amalie Steenholdt Fabricius Krogh er en mur\n",
      "Louis Bertelsen Joensen Terkelsen mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod England.\n"
     ]
    }
   ],
   "source": [
    "# patterns can also take a list of patterns to replace from (which can be weighted using the\n",
    "# patterns_prob argument. The pattern to use is sampled for each entity. \n",
    "# This setting is especially useful for finetuning models.\n",
    "multiple_pats = create_pers_augmenter(dk_name_dict, \n",
    "                                      force_pattern_size=True,\n",
    "                                      keep_name=False,\n",
    "                                      patterns=[\"fn,ln\", \"abbpunct,ln\", \"fn,ln,ln,ln\"])\n",
    "augmented_docs = augment_texts(texts, multiple_pats)\n",
    "for d in augmented_docs:\n",
    "    print(next(d).y.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with the options for `create_pers_augmenter` to get a feeling for how it works and check out the docs.\n",
    "\n",
    "The main strength of making the augmenters work with SpaCy is that we ensure that the spans of the augmented data still has the correct tags even though we add or remove words. This allows us to use them with gold-standard tagged datasets such as DaNE and use them for both training and evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Hans Christian Andersen, dansk) \t (Nadja Mønster, dansk)\n",
      "(Schmeichel,) \t (L Juhl,)\n",
      "(Peter Schmeichel, danske, England) \t (Kresten Huynh, danske, England)\n"
     ]
    }
   ],
   "source": [
    "docs = nlp.pipe(texts)\n",
    "augmented_docs = augment_texts(texts, multiple_pats)\n",
    "\n",
    "# Check that the added/removed PER entities are still tagged as entities\n",
    "for doc, aug_doc in zip(docs, augmented_docs):\n",
    "    print(doc.ents, \"\\t\", next(aug_doc).y.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributing\n",
    "\n",
    "We highly encourage others to contribute more augmenters that cover a wider range of use cases. For inspiration on how to make your own, checkout the source code for the ones included in `DaCy` in the `dacy/augmenters` folder and SpaCy's documentation [here](https://spacy.io/usage/training#data-augmentation). If you have a good idea for one or encounter any problems, please open an issue or write on the discussion board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69f6eb96a37a43867553731f8edb0a55a5852b5c9c304e04d7bd7872e5b1c11d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('dacy': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}