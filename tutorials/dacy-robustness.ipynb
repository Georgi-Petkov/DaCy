{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness Checking\n",
    "This tutorial walks through how to use `DaCy`/`SpaCy` augmenters to evalutate robustness of any NLP model. We'll start out by evaluating SpaCy and DaCy small on the testset of DaNE, before showing how to use this framework on any other type of model using DaNLP's BERT as an example. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import dacy\n",
    "import pandas as pd\n",
    "\n",
    "from spacy.training import Corpus, Example\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "from dacy.score import score\n",
    "from dacy.datasets import dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dane(splits=[\"test\"])\n",
    "spacy_small = spacy.load(\"da_core_news_sm\")\n",
    "# small/medium/large can be used instead of da_dacy_SIZE_tft-0.0.0\n",
    "dacy_small = dacy.load(\"small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating models already in the `SpaCy` framework is very straightforward. Simply call the `score` function on your nlp pipeline and choose which metrics you want to calculate performance for. `score` is a wrapper for `SpaCy.scorer.Scorer` that outputs a nicely formatted dataframe. `score` calculates performance for NER, POS, tokenization, and dependency parsing by default, which can be changed with the score_fn argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_baseline = score(test, apply_fn=spacy_small, score_fn=[\"ents\", \"pos\"])\n",
    "dacy_baseline = score(test, apply_fn=dacy_small, score_fn=[\"ents\", \"pos\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dacy_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain performance estimates on augmented data, simply provide the augmenter(s) in the `augmenters` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dacy.augmenters import create_pers_augmenter\n",
    "from dacy.datasets import male_names\n",
    "from spacy.training.augment import create_lower_casing_augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_aug = create_lower_casing_augmenter(level=1)\n",
    "male_name_dict = male_names()\n",
    "# Augmenter that replaces names with random Danish male names. Keep the format of the name as is (force_pattern_size=False)\n",
    "# but replace the name with one of the two defined patterns\n",
    "male_aug = create_pers_augmenter(male_name_dict, \n",
    "                                 patterns=[\"fn,ln\",\"abbpunct,ln\"], \n",
    "                                 force_pattern_size=False,\n",
    "                                 keep_name=False)\n",
    "\n",
    "spacy_aug = score(test, \n",
    "                  apply_fn=spacy_small,\n",
    "                  score_fn=[\"ents\", \"pos\"],\n",
    "                  augmenters=[lower_aug, male_aug])\n",
    "dacy_aug = score(test,\n",
    "                 apply_fn=dacy_small,\n",
    "                 score_fn=[\"ents\", \"pos\"],\n",
    "                 augmenters=[lower_aug, male_aug])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([spacy_baseline, spacy_aug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dacy_baseline, dacy_aug])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second row, we see that `SpaCy small` is very vulnerable to lower casing as NER recall drops from 0.63 to 0.09. `DaCy small` is slightly more robust lower casing, but still suffers. Changing names also leads to a drop in performance for both models. \n",
    "\n",
    "To better estimate the effect of stochastic augmenters such as those changing names or adding keystroke errors we can use the `k` argument in `score` to run the augmenter multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dacy.augmenters import create_keyboard_augmenter\n",
    "\n",
    "key_05_aug = create_keyboard_augmenter(doc_level=1, char_level=0.05, keyboard=\"QWERTY_DA\")\n",
    "\n",
    "spacy_key = score(test, \n",
    "                  apply_fn=spacy_small,\n",
    "                  score_fn=[\"ents\", \"pos\"],\n",
    "                  augmenters=[key_05_aug],\n",
    "                  k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this manner, evaluating performance on augmented data for SpaCy pipelines is as easy as defining the augmenters and calling a single function. In the `dacy_paper_replication.py` script you can find the exact script used to evaluate the robustness of Danish NLP models in the DaCy paper.\n",
    "\n",
    "# Custom Models\n",
    "Evaluating models not in the `SpaCy` framework requires to write an `apply_fn` that takes a Spacy `Example` as input, applies your model to it, turns it into a `Doc`, and returns an `Example`. \n",
    "\n",
    "The following shows how to write one for DanNLP's BERT named entity recognition model. `add_iob` adds the entities to the predicted `Doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.models import load_bert_ner_model\n",
    "from typing import List\n",
    "from spacy.lang.da import Danish\n",
    "# load model\n",
    "bert_model = load_bert_ner_model()\n",
    "# instantiate empty Dansih Spacy NLP pipeline\n",
    "nlp_da = Danish()\n",
    "\n",
    "def apply_bert_model(example: Example) -> Example:\n",
    "    doc = nlp_da(example.reference.text)\n",
    "    tokens, labels = bert_model.predict([t.text for t in doc])\n",
    "    doc = add_iob(doc, labels)\n",
    "    return Example(doc, example.reference)\n",
    "\n",
    "\n",
    "def add_iob(doc: Doc, iob: List[str]) -> Doc:\n",
    "    \"\"\"Add iob tags to Doc\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): A SpaCy doc\n",
    "        iob (List[str]): a list of tokens on the IOB format\n",
    "\n",
    "    Returns:\n",
    "        Doc: A doc with the spans to the new IOB\n",
    "    \"\"\"\n",
    "    ent = []\n",
    "    for i, label in enumerate(iob):\n",
    "\n",
    "        # turn OOB labels into spans\n",
    "        if label == \"O\":\n",
    "            continue\n",
    "        iob_, ent_type = label.split(\"-\")\n",
    "        if (i - 1 >= 0 and iob_ == \"I\" and iob[i - 1] == \"O\") or (\n",
    "            i == 0 and iob_ == \"I\"\n",
    "        ):\n",
    "            iob_ = \"B\"\n",
    "        if iob_ == \"B\":\n",
    "            start = i\n",
    "        if i + 1 >= len(iob) or iob[i + 1].split(\"-\")[0] != \"I\":\n",
    "            ent.append(Span(doc, start, i + 1, label=ent_type))\n",
    "    doc.set_ents(ent)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "danlp_bert = score(test, apply_fn=apply_bert_model, score_fn=[\"ents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are in doubt how to create an apply function for your model you can find inspiration in `papers/DaCy../apply_fns`. This folder contains apply functions for DaNLP's BERT, Flair, NERDA, and Polyglot. Otherwise, feel free to open an issue on Github. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69f6eb96a37a43867553731f8edb0a55a5852b5c9c304e04d7bd7872e5b1c11d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('dacy': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}